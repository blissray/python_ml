{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['filenames', 'target_names', 'description', 'data', 'target', 'DESCR'])\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "(18846,)\n",
      "(18846,)\n"
     ]
    }
   ],
   "source": [
    "print(news.keys())\n",
    "print(news['target_names'])\n",
    "print(news['filenames'].shape)\n",
    "print(news['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(news.data)*SPLIT_PERC)\n",
    "X_train = news.data[:split_size]\n",
    "X_test = news.data[split_size:]\n",
    "y_train = news.target[:split_size]\n",
    "y_test = news.target[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_2 = Pipeline([\n",
    "    ('vect', HashingVectorizer(non_negative=True)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85782493  0.85725657  0.84664367  0.85911382  0.8458477 ]\n",
      "Mean score: 0.853 (+/-0.003)\n",
      "[ 0.75543767  0.77659857  0.77049615  0.78508888  0.76200584]\n",
      "Mean score: 0.770 (+/-0.005)\n",
      "[ 0.84482759  0.85990979  0.84558238  0.85990979  0.84213319]\n",
      "Mean score: 0.850 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "clfs = [clf_1, clf_2, clf_3]\n",
    "for clf in clfs:\n",
    "    evaluate_cross_validation(clf, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86100796  0.8718493   0.86203237  0.87291059  0.8588485 ]\n",
      "Mean score: 0.865 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_4, news.data, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open('./data/stopwords_en.txt', 'r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sometime', 'below', 'elsewhere', 'found', 'seem', 'never', 'thereby', 'own', 'along', 'there', 'somehow', 'nothing', 'would', 'themselves', 'on', 'such', 'hasnt', 'describe', 'thereupon', 'whatever', 'whether', 'seems', 'within', 'where', 'cant', 'thick', 'his', 'although', 'could', 'ie', 'however', 'throughout', 'many', 'get', 'sincere', 'thence', 'well', 'their', 'of', 'very', 'hereupon', 'but', 'might', 'per', 'indeed', 'show', 'thus', 'everything', 'against', 'he', 'into', 'other', 'to', 'front', 'everywhere', 'whence', 'more', 'those', 'most', 'will', 'both', 'whereby', 'either', 'myself', 'therein', 'these', 'thru', 'toward', 'move', 'two', 'a', 'cannot', 'every', 'be', 'become', 'ltd', 'rather', 'anywhere', 'us', 'few', 'wherever', 'except', 'hereafter', 'here', 'becoming', 'it', 'until', 'moreover', 'they', 'put', 'forty', 'see', 'nor', 'through', 'whereafter', 'everyone', 'detail', 'onto', 'etc', 'therefore', 'any', 'not', 'them', 'anything', 'mine', 'find', 'much', 'some', 'upon', 'yourselves', 'nobody', 'yours', 'part', 'anyhow', 'behind', 'several', 'himself', 'being', 'almost', 'beside', 'side', 'the', 'towards', 'at', 'yet', 'least', 'across', 'bill', 'because', 'third', 'by', 'under', 'what', 'last', 'amoungst', 'interest', 'three', 'same', 'namely', 'ourselves', 'amount', 'an', 'after', 'fill', 'mill', 'nevertheless', 'or', 'ours', 'while', 'serious', 'than', 'whither', 'co', 'are', 'else', 'may', 'twenty', 'ten', 'beyond', 'why', 'is', 'go', 'before', 'inc', 'my', 'once', 'became', 'hers', 'done', 'afterwards', 'fify', 'next', 'me', 'otherwise', 'due', 'meanwhile', 'since', 'our', 'sixty', 'somewhere', 'perhaps', 'were', 'give', 'as', 'bottom', 'former', 'without', 'had', 'cry', 'besides', 'we', 'each', 'i', 'latterly', 'one', 'via', 'above', 'also', 'if', 'hereby', 'none', 'all', 'has', 'no', 'so', 'someone', 'wherein', 'whom', 'call', 'further', 'hundred', 'enough', 'whoever', 'sometimes', 'how', 'been', 'when', 'with', 'becomes', 'herself', 'seemed', 'system', 'latter', 'top', 'something', 'thereafter', 'though', 'must', 'often', 'fire', 'too', 'seeming', 'anyone', 'anyway', 'up', 'she', 'do', 're', 'six', 'you', 'and', 'about', 'mostly', 'neither', 'now', 'whereupon', 'made', 'your', 'four', 'itself', 'off', 'whenever', 'even', 'always', 'please', 'only', 'already', 'was', 'herein', 'amongst', 'noone', 'couldnt', 'that', 'eleven', 'empty', 'around', 'together', 'con', 'five', 'twelve', 'yourself', 'am', 'less', 'eight', 'have', 'her', 'its', 'take', 'thin', 'should', 'un', 'again', 'then', 'which', 'alone', 'whose', 'can', 'during', 'for', 'whole', 'whereas', 'first', 'keep', 'name', 'among', 'between', 'eg', 'nine', 'another', 'in', 'over', 'who', 'ever', 'still', 'out', 'this', 'from', 'fifteen', 'others', 'back', 'nowhere', 'beforehand', 'de', 'him', 'formerly', 'down', 'hence', 'full'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_stop_words()\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_5 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",    \n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88116711  0.89519767  0.88325816  0.89227912  0.88113558]\n",
      "Mean score: 0.887 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_5, news.data, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_7 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9204244   0.91960732  0.91828071  0.92677103  0.91854603]\n",
      "Mean score: 0.921 (+/-0.002)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_7, news.data, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print (\"Accuracy on training set:\")\n",
    "    print (clf.score(X_train, y_train))\n",
    "    print (\"Accuracy on testing set:\")\n",
    "    print (clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print (\"Classification Report:\")\n",
    "    print (metrics.classification_report(y_test, y_pred))\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.996957690675\n",
      "Accuracy on testing set:\n",
      "0.917869269949\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.88      0.91       216\n",
      "          1       0.85      0.85      0.85       246\n",
      "          2       0.91      0.84      0.87       274\n",
      "          3       0.81      0.86      0.83       235\n",
      "          4       0.88      0.90      0.89       231\n",
      "          5       0.89      0.91      0.90       225\n",
      "          6       0.88      0.80      0.84       248\n",
      "          7       0.92      0.93      0.93       275\n",
      "          8       0.96      0.98      0.97       226\n",
      "          9       0.97      0.94      0.96       250\n",
      "         10       0.97      1.00      0.98       257\n",
      "         11       0.97      0.97      0.97       261\n",
      "         12       0.90      0.91      0.91       216\n",
      "         13       0.94      0.95      0.95       257\n",
      "         14       0.94      0.97      0.95       246\n",
      "         15       0.90      0.96      0.93       234\n",
      "         16       0.91      0.97      0.94       218\n",
      "         17       0.97      0.99      0.98       236\n",
      "         18       0.95      0.91      0.93       213\n",
      "         19       0.86      0.78      0.82       148\n",
      "\n",
      "avg / total       0.92      0.92      0.92      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[190   0   0   0   1   0   0   0   0   1   0   0   0   1   0   9   2   0\n",
      "    0  12]\n",
      " [  0 208   5   3   3  13   4   0   0   0   0   1   3   2   3   0   0   1\n",
      "    0   0]\n",
      " [  0  11 230  22   1   5   1   0   1   0   0   0   0   0   1   0   1   0\n",
      "    1   0]\n",
      " [  0   6   6 202   9   3   4   0   0   0   0   0   4   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   3   4 208   1   5   0   0   0   2   0   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   9   2   2   1 205   0   1   1   0   0   0   0   2   1   0   0   1\n",
      "    0   0]\n",
      " [  0   2   3  10   6   0 199  14   1   2   0   1   5   2   2   0   0   1\n",
      "    0   0]\n",
      " [  0   1   1   1   1   0   6 257   4   1   0   0   0   1   0   0   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   2 221   0   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   0   2 236   5   0   1   3   0   1   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0 256   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   0   1   0   0   0 254   0   1   0   0   3   0\n",
      "    1   0]\n",
      " [  0   1   0   1   5   1   3   1   0   2   1   1 197   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   1   0   0   0   0   0   0   2   2 245   3   0   1   0\n",
      "    0   1]\n",
      " [  0   2   0   0   1   0   0   1   0   0   0   0   0   1 238   0   1   0\n",
      "    1   1]\n",
      " [  1   0   1   2   0   0   0   1   0   0   0   1   1   0   1 225   0   1\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   1   0   1   0   0   1   0   0   0   0 212   0\n",
      "    2   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 234\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   2   1   1   0   1   7   3\n",
      "  193   4]\n",
      " [  9   0   0   0   0   1   0   0   0   1   0   0   0   0   0  13   4   1\n",
      "    4 115]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01_introduction.ma',\n",
       " '01apr93.17160985.0059',\n",
       " '01c8',\n",
       " '01f6',\n",
       " '01h0',\n",
       " '01ll',\n",
       " '01ne',\n",
       " '01ob',\n",
       " '01vl2',\n",
       " '01ya']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_7.named_steps['vect'].get_feature_names()[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9204244   0.91960732  0.91828071  0.92677103  0.91854603]\n",
      "Mean score: 0.921 (+/-0.002)\n",
      "[ 0.91352785  0.91615813  0.91509684  0.92464845  0.91615813]\n",
      "Mean score: 0.917 (+/-0.002)\n"
     ]
    }
   ],
   "source": [
    "clf_8 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                tokenizer=LemmaTokenizer(), # Stemming 을 위한 WordNet 기반 Tokenlizer 추가\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",         \n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "])\n",
    "\n",
    "evaluate_cross_validation(clf_7, news.data, news.target, 5)\n",
    "evaluate_cross_validation(clf_8, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.99596717136\n",
      "Accuracy on testing set:\n",
      "0.911926994907\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.88      0.91       216\n",
      "          1       0.84      0.82      0.83       246\n",
      "          2       0.89      0.84      0.87       274\n",
      "          3       0.81      0.85      0.83       235\n",
      "          4       0.89      0.88      0.88       231\n",
      "          5       0.87      0.92      0.90       225\n",
      "          6       0.87      0.80      0.83       248\n",
      "          7       0.92      0.91      0.92       275\n",
      "          8       0.94      0.97      0.96       226\n",
      "          9       0.96      0.94      0.95       250\n",
      "         10       0.96      1.00      0.98       257\n",
      "         11       0.96      0.98      0.97       261\n",
      "         12       0.90      0.92      0.91       216\n",
      "         13       0.94      0.95      0.94       257\n",
      "         14       0.95      0.95      0.95       246\n",
      "         15       0.90      0.96      0.93       234\n",
      "         16       0.90      0.97      0.93       218\n",
      "         17       0.97      0.99      0.98       236\n",
      "         18       0.94      0.90      0.92       213\n",
      "         19       0.89      0.76      0.82       148\n",
      "\n",
      "avg / total       0.91      0.91      0.91      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[191   0   0   0   1   0   0   0   0   1   0   0   0   1   0  11   2   0\n",
      "    0   9]\n",
      " [  0 201   7   4   3  15   2   0   0   0   0   3   3   2   4   0   0   2\n",
      "    0   0]\n",
      " [  0   9 231  22   2   6   1   1   0   0   0   0   0   0   0   0   1   0\n",
      "    1   0]\n",
      " [  1   6   9 199   7   5   5   0   0   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   5   5 203   0   8   0   0   0   2   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   9   1   0   0 208   0   1   1   1   0   0   0   2   1   0   0   1\n",
      "    0   0]\n",
      " [  0   4   5   7   4   0 199  12   0   3   1   1   6   2   1   0   1   1\n",
      "    1   0]\n",
      " [  0   1   0   1   1   1   7 250   8   1   0   0   1   1   0   0   2   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   1   1   3 219   1   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   1   0   2 234   7   0   0   3   0   0   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 257   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   0   0 256   0   1   0   0   2   0\n",
      "    0   0]\n",
      " [  0   1   0   3   5   0   1   1   1   1   1   1 198   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   1   0   0   0   0   0   0   2   2 243   4   0   2   0\n",
      "    0   1]\n",
      " [  0   2   0   0   1   1   1   2   0   0   0   1   0   1 234   0   1   0\n",
      "    1   1]\n",
      " [  1   1   1   3   0   0   1   1   0   0   0   0   0   1   0 225   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   1   0   0   0   0   2   0   0   0   0 211   0\n",
      "    3   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 234\n",
      "    1   0]\n",
      " [  1   0   0   1   0   0   1   0   0   0   0   2   1   1   1   1   7   3\n",
      "  191   3]\n",
      " [  9   0   0   0   1   0   0   0   1   1   0   0   0   0   0  13   4   1\n",
      "    5 113]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_8, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
